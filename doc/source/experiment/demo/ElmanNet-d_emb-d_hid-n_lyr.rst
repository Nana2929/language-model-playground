Elman Net: ``d_emb`` vs ``d_hid`` vs ``n_lyr``
==============================================

Abstract
--------
This goal of this experiment is to show how Elman Net language model's structure hyperparameters affect training loss and perplexity.
We found that

- Increasing ``d_emb`` from ``10`` to ``100`` makes training loss and perplexity lower.
- Increasing ``d_hid`` from ``10`` to ``100`` makes training loss and perplexity lower.
- When ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``2`` (or ``3``) makes training loss and perplexity lower.
- No overfitting was observed.
- :math:`100\%` accuracy on training set is possible.
- Performance are really bad for validation and test sets.
  There might be problems related to dataset design philosophy.

Environment setup
-----------------
We ran experiments on Nvidia RTX 2070S.
CUDA version is ``11.4`` and CUDA driver version is ``470.129.06``.

Experiment setup
----------------
We changed the values of ``d_emb``, ``d_hid`` and ``n_lyr`` and recorded training loss and perplexity.
Parameters and their values are listed below.

+-----------+-----------------------+
| Name      | Values                |
+===========+=======================+
| ``d_emb`` | :math:`\set{10, 100}` |
+-----------+-----------------------+
| ``d_hid`` | :math:`\set{10, 100}` |
+-----------+-----------------------+
| ``n_lyr`` | :math:`\set{1, 2, 3}` |
+-----------+-----------------------+

Tokenizer settings
~~~~~~~~~~~~~~~~~~
We used character tokenizer :py:class:`~lmp.tknzr.CharTknzr`.
We used :doc:`lmp.script.train_tknzr </script/train_tknzr>` to train our tokenizer.
Script was executed as below:

.. code-block:: shell

  python -m lmp.script.train_tknzr character \
    --dset_name demo \
    --exp_name demo_tknzr \
    --is_uncased \
    --max_vocab -1 \
    --min_count 0 \
    --ver train

Model training settings
~~~~~~~~~~~~~~~~~~~~~~~
We trained Elman Net language model :py:class:`~lmp.model.ElmanNet` with different model structure hyperparameters.
We used :doc:`lmp.script.train_model </script/train_model>` to train language models.
Script was executed as below:

.. code-block:: shell

  python -m lmp.script.train_model Elman-Net \
    --dset_name demo \
    --batch_size 32 \
    --beta1 0.9 \
    --beta1 0.999 \
    --ckpt_step 500 \
    --d_emb D_EMB \
    --d_hid D_HID \
    --dset_name demo \
    --eps 1e-8 \
    --exp_name EXP_NAME \
    --init_lower -0.1 \
    --init_upper 0.1 \
    --label_smoothing 0.0 \
    --log_step 100 \
    --lr 1e-3 \
    --max_norm 1 \
    --max_seq_len 35 \
    --n_lyr N_LYR \
    --p_emb 0.0 \
    --p_hid 0.0 \
    --seed 42 \
    --stride 35 \
    --tknzr_exp_name demo_tknzr \
    --total_step 30000 \
    --ver train \
    --warmup_step 5000 \
    --weight_decay 0.0

Model evaluation settings
~~~~~~~~~~~~~~~~~~~~~~~~~
We evaluated language models using :doc:`lmp.script.eval_dset_ppl </script/eval_dset_ppl>`.
Script was executed as below:

.. code-block:: shell

  python -m lmp.script.eval_dset_ppl demo \
    --batch_size 512 \
    --exp_name EXP_NAME \
    --first_ckpt 0 \
    --last_ckpt -1 \
    --seed 42 \
    --ver VER

Experiment results
------------------
All results were logged on tensorboard.
You can launch tensorboard with the script

.. code-block:: shell

  pipenv run tensorboard

Training loss
~~~~~~~~~~~~~

+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| ``d_emb`` | ``d_hid`` | ``n_lyr`` | 5k steps   | 10k steps  | 15k steps  | 20k steps  | 25k steps  | 30k steps  |
+===========+===========+===========+============+============+============+============+============+============+
| 10        | 10        | 1         | 0.7045     | 0.3986     | 0.3789     | 0.3725     | 0.3659     | 0.3637     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 10        | 10        | 2         | 1.064      | 0.4145     | 0.389      | 0.3768     | 0.3734     | 0.3711     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 10        | 10        | 3         | 2.496      | 0.6864     | 0.5211     | 0.4777     | 0.4678     | 0.4645     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 10        | 100       | 1         | 0.5376     | 0.3764     | 0.3696     | 0.3509     | 0.3441     | 0.3423     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 10        | 100       | 2         | 1.255      | 0.3774     | 0.37       | 0.369      | 0.3688     | 0.3681     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 10        | 100       | 3         | 0.5312     | 0.4244     | 0.4145     | 0.3726     | 0.3703     | 0.3656     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 10        | 1         | 0.3768     | 0.3708     | 0.3679     | 0.3439     | 0.3355     | 0.3322     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 10        | 2         | 0.379      | 0.3697     | 0.369      | 0.3683     | 0.368      | 0.3675     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 10        | 3         | 0.3784     | 0.3688     | 0.3668     | 0.3658     | 0.3653     | 0.3653     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 100       | 1         | 0.3099     | 0.2765     | 0.2651     | 0.255      | 0.2443     | 0.2409     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 100       | 2         | 0.3016     | **0.2696** | **0.2574** | **0.2455** | **0.2373** | 0.2363     |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+
| 100       | 100       | 3         | **0.2950** | 0.2723     | 0.2604     | 0.2462     | 0.2387     | **0.2361** |
+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+

Observation 1: Increasing ``d_emb`` from ``10`` to ``100`` makes training loss smaller.
***************************************************************************************
By fixing ``d_hid`` and ``n_lyr``, we can compare training loss for ``d_emb = 10`` and ``d_emb = 100``.
All comparisons (:math:`\dfrac{36}{36}`) show that training loss is smaller when increasing ``d_emb`` from ``10`` to ``100``.

Observation 2: Increasing ``d_hid`` from ``10`` to ``100`` makes training loss smaller.
***************************************************************************************
By fixing ``d_emb`` and ``n_lyr``, we can compare training loss for ``d_hid = 10`` and ``d_hid = 100``.
All comparisons (:math:`\dfrac{36}{36})` show that training loss is smaller when increasing ``d_hid`` from ``10`` to ``100``.

Observation 3: When ``d_emb = 10``, increasing ``n_lyr`` from ``1`` to ``2`` makes training loss larger.
********************************************************************************************************
By fixing ``d_emb = 10`` and ``d_hid``, we can compare training loss for ``n_lyr = 1`` and ``n_lyr = 2``.
All comparisons (:math:`\dfrac{12}{12})` show that training loss is larger when increasing ``n_lyr`` from ``1`` to ``2``.

Observation 4: When ``d_emb = 10``, increasing ``n_lyr`` from ``1`` to ``3`` makes training loss larger.
********************************************************************************************************
By fixing ``d_emb = 10`` and ``d_hid``, we can compare training loss for ``n_lyr = 1`` and ``n_lyr = 3``.
All comparisons (:math:`\dfrac{12}{12})` show that training loss is larger when increasing ``n_lyr`` from ``1`` to ``3``.

Observation 5: When ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``2`` in general makes training loss smaller.
*********************************************************************************************************************
By fixing ``d_emb = 100`` and ``d_hid``, we can compare training loss for ``n_lyr = 1`` and ``n_lyr = 2``.
:math:`7` out of :math:`12` comparisons show that training loss is smaller when increasing ``n_lyr`` from ``1`` to ``2``.
Thus we conclude that when ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``2`` in general makes training loss smaller.

Observation 6: When ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``3`` in general makes training loss smaller.
*********************************************************************************************************************
By fixing ``d_emb = 100`` and ``d_hid``, we can compare training loss for ``n_lyr = 1`` and ``n_lyr = 3``.
:math:`8` out of :math:`12` comparisons show that training loss is smaller when increasing ``n_lyr`` from ``1`` to ``3``.
Thus we conclude that when ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``3`` in general makes training loss smaller.

Observation 7: Increasing ``n_lyr`` must also increase ``d_emb``.
*****************************************************************
Combining observations from 3 to 6 we conclude that when increasing ``n_lyr`` one have to increase ``d_emb`` together to make training loss smaller.

Observation 8: Minimum loss is achieved when ``d_emb = 100`` and ``n_lyr = 2 or 3``.
************************************************************************************
From tensorboard logs we see the two lines are almost identical.

Observation 9: Training loss is still decreasing in all configuration.
**********************************************************************
All comparisons (:math:`\dfrac{60}{60}`) show that training loss is still decreasing no matter which configuration is used.
This suggest that further training may be required.

Perplexity
~~~~~~~~~~

+-----------+-----------+-----------+----------------------------------+----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+
| ``d_emb`` | ``d_hid`` | ``n_lyr`` | 5k steps                         | 10k steps                        | 15k steps                         | 20k steps                         | 25k steps                         | 30k steps                         |
|           |           |           +----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
|           |           |           | train    | valid     | test      | train     | valid     | test     | train     | valid     | test      | train     | valid     | test      | train     | valid     | test      | train     | valid     | test      |
+===========+===========+===========+==========+===========+===========+===========+===========+==========+===========+===========+===========+===========+===========+===========+===========+===========+===========+===========+===========+===========+
| 10        | 10        | 1         | 1.179    | 1.18      | 1.181     | 1.068     | 1.066     | 1.064    | 1.06      | 1.052     | 1.053     | 1.058     | 1.047     | 1.048     | 1.056     | 1.045     | 1.046     | 1.056     | 1.044     | 1.047     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 10        | 10        | 2         | 1.224    | 1.233     | 1.219     | 1.069     | 1.071     | 1.071    | 1.055     | 1.048     | 1.049     | 1.054     | 1.044     | 1.046     | 1.054     | 1.042     | 1.042     | 1.054     | 1.052     | 1.048     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 10        | 10        | 3         | 1.229    | 1.229     | 1.229     | 1.116     | 1.115     | 1.114    | 1.059     | 1.06      | 1.059     | 1.054     | 1.053     | 1.051     | 1.054     | 1.052     | 1.05      | 1.054     | 1.052     | 1.05      |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 10        | 100       | 1         | 1.13     | 1.119     | 1.117     | 1.058     | 1.047     | 1.048    | 1.054     | 1.041     | 1.039     | 1.052     | 1.037     | 1.037     | 1.051     | 1.038     | 1.036     | 1.051     | 1.038     | 1.036     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 10        | 100       | 2         | 1.272    | 1.253     | 1.252     | 1.058     | 1.044     | 1.049    | 1.055     | 1.042     | 1.044     | 1.054     | 1.04      | 1.041     | 1.054     | 1.039     | 1.039     | 1.054     | 1.04      | 1.039     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 10        | 100       | 3         | 1.131    | 1.129     | 1.127     | 1.08      | 1.064     | 1.069    | 1.076     | 1.059     | 1.06      | 1.056     | 1.04      | 1.041     | 1.054     | 1.041     | 1.04      | 1.054     | 1.042     | 1.039     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 10        | 1         | 1.057    | 1.047     | 1.046     | 1.054     | 1.044     | 1.041    | 1.054     | 1.04      | 1.041     | 1.053     | 1.044     | 1.043     | 1.053     | 1.044     | 1.039     | 1.053     | 1.043     | 1.04      |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 10        | 2         | 1.056    | 1.039     | 1.04      | 1.054     | 1.039     | 1.038    | 1.054     | 1.043     | 1.041     | 1.054     | 1.043     | 1.042     | 1.054     | 1.044     | 1.042     | 1.054     | 1.042     | 1.041     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 10        | 3         | 1.056    | 1.039     | 1.039     | 1.055     | 1.039     | 1.041    | 1.054     | 1.038     | 1.038     | 1.054     | 1.037     | 1.038     | 1.054     | 1.037     | 1.038     | 1.054     | 1.037     | 1.039     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 100       | 1         | 1.053    | **1.032** | 1.035     | 1.048     | 1.027     | 1.033    | 1.045     | 1.025     | 1.03      | 1.042     | 1.023     | 1.03      | 1.039     | 1.021     | 1.029     | 1.037     | 1.021     | 1.029     |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 100       | 2         | **1.05** | **1.032** | **1.033** | **1.045** | 1.026     | **1.03** | 1.042     | 1.023     | 1.028     | 1.038     | 1.021     | **1.027** | **1.034** | **1.019** | **1.026** | 1.034     | **1.018** | **1.026** |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| 100       | 100       | 3         | **1.05** | **1.032** | **1.033** | **1.045** | **1.025** | **1.03** | **1.04**  | **1.022** | **1.027** | **1.036** | **1.02**  | **1.027** | **1.034** | **1.019** | **1.026** | **1.033** | **1.018** | **1.026** |
+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+

Observation 1: Increasing ``d_emb`` from ``10`` to ``100`` makes perplexity smaller.
************************************************************************************
By fixing ``d_hid`` and ``n_lyr``, we can compare perplexity for ``d_emb = 10`` and ``d_emb = 100``.
Almost all comparisons (:math:`\dfrac{100}{108}`) show that perplexity is smaller when increasing ``d_emb`` from ``10`` to ``100``.

Observation 2: Increasing ``d_hid`` from ``10`` to ``100`` makes perplexity smaller.
************************************************************************************
By fixing ``d_emb`` and ``n_lyr``, we can compare perplexity for ``d_hid = 10`` and ``d_hid = 100``.
Almost all comparisons (:math:`\dfrac{96}{108}`) show that perplexity is smaller when increasing ``d_hid`` from ``10`` to ``100``.

Observation 3: When ``d_emb = 10``, increasing ``n_lyr`` from ``1`` to ``2`` in general makes perplexity larger.
****************************************************************************************************************
By fixing ``d_emb = 10`` and ``d_hid``, we can compare perplexity for ``n_lyr = 1`` and ``n_lyr = 2``.
Most comparisons (:math:`\dfrac{25}{36}`) show that perplexity is larger when increasing ``n_lyr`` from ``1`` to ``2``.

Observation 4: When ``d_emb = 10``, increasing ``n_lyr`` from ``1`` to ``3`` makes perplexity larger.
*****************************************************************************************************
By fixing ``d_emb = 10`` and ``d_hid``, we can compare perplexity for ``n_lyr = 1`` and ``n_lyr = 3``.
Almost all comparisons (:math:`\dfrac{32}{36}`) show that perplexity is larger when increasing ``n_lyr`` from ``1`` to ``3``.

Observation 5: When ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``2`` in general makes perplexity smaller.
******************************************************************************************************************
By fixing ``d_emb = 100`` and ``d_hid``, we can compare perplexity for ``n_lyr = 1`` and ``n_lyr = 2``.
Most comparisons (:math:`\dfrac{27}{36}`) show that perplexity is smaller when increasing ``n_lyr`` from ``1`` to ``2``.

Observation 6: When ``d_emb = 100``, increasing ``n_lyr`` from ``1`` to ``3`` makes perplexity smaller.
*******************************************************************************************************
By fixing ``d_emb = 100`` and ``d_hid``, we can compare perplexity for ``n_lyr = 1`` and ``n_lyr = 3``.
Almost all comparisons (:math:`\dfrac{30}{36}`) show that perplexity is smaller when increasing ``n_lyr`` from ``1`` to ``3``.

Observation 7: No overfitting seems to happen.
**********************************************
On validation and test set, most comparisons (:math:`\dfrac{84}{120}`) show that perplexity is still decreasing in most configurations.

Observation 8: Minimum perplexity on training set is achieved at ``30k`` step when ``d_emb = 100``, ``d_hid = 100`` and ``n_lyr = 3``.
**************************************************************************************************************************************

- On training set, minimum perplexity :math:`1.033` is achieved at ``30k`` step when ``d_emb = 100``, ``d_hid = 100`` and ``n_lyr = 3``.
- On validation set, minimum perplexity :math:`1.018` is achieved at ``30k`` step when ``d_emb = 100``, ``d_hid = 100`` and ``n_lyr = 3``.
- On training set, minimum perplexity :math:`1.026` is achieved at ``25k`` and ``30k`` step when ``d_emb = 100``, ``d_hid = 100`` and ``n_lyr = 3``.

Observation 9: Only when setting ``d_emb = 100`` and ``d_hid = 100`` perplexity is lower than :math:`1.05`.
***********************************************************************************************************
Later in the accuracy experiments we see that only when perplexity is lower than :math:`1.05` accuracy can be higher than :math:`90 \%`.

Accuracy
--------
We use the following script to calculate accuracy on demo dataset:

.. code-block:: python

  import re

  import torch

  import lmp.dset
  import lmp.infer
  import lmp.model
  import lmp.script
  import lmp.tknzr
  import lmp.util.model
  import lmp.util.tknzr

  device = torch.device('cuda')
  tknzr = lmp.util.tknzr.load(exp_name='demo_tknzr')
  for d_emb in [10, 100]:
    for d_hid in [10, 100]:
      for n_lyr in [1, 2, 3]:
        for ckpt in [5000, 10000, 15000, 20000, 25000, 30000]:
          for ver in lmp.dset.DemoDset.vers:
            dset = lmp.dset.DemoDset(ver=ver)
            exp_name = f'demo-d_emb-{d_emb}-d_hid-{d_hid}-n_lyr-{n_lyr}'
            model = lmp.util.model.load(exp_name=exp_name, ckpt=ckpt).to(device)
            infer = lmp.infer.Top1Infer(max_seq_len=35)

            correct = 0
            for spl in dset:
              match = re.match(r'If you add (\d+) to (\d+) you get (\d+) .', spl)
              input = f'If you add {match.group(1)} to {match.group(2)} you get '

              output = infer.gen(model=model, tknzr=tknzr, txt=input)

              if input + output == spl:
                correct += 1

            print(f'{exp_name}, ckpt: {ckpt}, ver: {ver}, acc: {correct / len(dset) * 100 :.2f}%')


+-----------+-----------+-----------+-------------------------------+------------------------------+------------------------------+------------------------------+---------------------------+-------------------------+
| ``d_emb`` | ``d_hid`` | ``n_lyr`` | 5k steps                      | 10k steps                    | 15k steps                    | 20k steps                    | 25k steps                 | 30k steps               |
|           |           |           +-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
|           |           |           | train     | valid     | test  | train     | valid     | test | train     | valid     | test | train     | valid     | test | train     | valid  | test | train   | valid  | test |
+===========+===========+===========+===========+===========+=======+===========+===========+======+===========+===========+======+===========+===========+======+===========+========+======+=========+========+======+
| 10        | 10        | 1         | 1.56      | 1.56      | 0     | 1.64      | 2         | 0    | 1.92      | 2         | 0    | 1.84      | 2         | 0    | 1.6       | 2      | 0    | 1.56    | 2      | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 10        | 10        | 2         | 0         | 0         | 0     | 1.36      | 1.36      | 0    | 1.36      | 1.36      | 0    | 1.96      | 1.96      | 0    | 1.88      | 1.68   | 0    | 2       | 1.8    | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 10        | 10        | 3         | 0         | 0         | 0     | 0         | 0         | 0    | 1.76      | 1.44      | 0    | 1.76      | 1.76      | 0    | 1.76      | 1.76   | 0    | 1.32    | 1.32   | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 10        | 100       | 1         | 1.92      | 1.92      | 0     | 1.96      | 1.96      | 0    | 1.96      | 1.96      | 0    | 3.04      | 2.8       | 0    | 2.76      | 2.92   | 0    | 4.6     | 3      | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 10        | 100       | 2         | 0         | 0         | 0     | 1.96      | 1.96      | 0    | 1.92      | 1.92      | 0    | 1.92      | 1.92      | 0    | 1.92      | 1.92   | 0    | 1.92    | 1.92   | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 10        | 100       | 3         | 1.76      | 1.76      | 0     | 1.92      | 1.92      | 0    | 1.76      | 1.76      | 0    | 1.76      | 1.76      | 0    | 1.92      | 1.92   | 0    | 2       | 1.96   | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 10        | 1         | 1.92      | 1.92      | 0     | 1.76      | 1.76      | 0    | 1.88      | 1.96      | 0    | 5.08      | 2.68      | 0    | 8.2       | 2.68   | 0    | 9.68    | 3      | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 10        | 2         | 1.4       | 1.4       | 0     | 1.96      | 1.96      | 0    | 1.64      | 1.72      | 0    | 1.88      | 1.88      | 0    | 1.96      | 1.96   | 0    | 1.96    | 1.96   | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 10        | 3         | 1.84      | 1.76      | 0     | 1.76      | 1.76      | 0    | 1.92      | 1.68      | 0    | 2         | 1.84      | 0    | 2         | 1.2    | 0    | 2       | 1.32   | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 100       | 1         | 15        | 2.12      | 0     | **74.96** | 13.44     | 0    | 69.52     | 20.84     | 0    | 87.52     | 17.12     | 0    | 98.92     | 19.60  | 0    | 99.24   | 20.88  | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 100       | 2         | 15.36     | 10.88     | 0     | 55.88     | **24.76** | 0    | 62.20     | **34.96** | 0    | 90.16     | 32.88     | 0    | 99.76     | 34.92  | 0    | 99.96   | 34.28  | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+
| 100       | 100       | 3         | **29.6**  | **22.64** | 0     | 46.72     | 19.72     | 0    | **83.20** | 30.28     | 0    | **97.32** | **33.60** | 0    | **99.84** | **35** | 0    | **100** | **35** | 0    |
+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+------+-----------+-----------+------+-----------+-----------+------+-----------+--------+------+---------+--------+------+

Observation 1: :math:`100\%` accuracy is possible on training set.
------------------------------------------------------------------
This is achieved using ``d_emb = 100``, ``d_hid = 100`` and ``n_lyr = 3``.

Observation 2: Models are not generalized.
------------------------------------------
Test set always have :math:`0\%` accuracy.
Validation set do not have accuracy higher than :math:`50\%`.
This might be the problem of dataset design.

Future work
-----------
We will try to fix demo dataset.

.. footbiliography::
